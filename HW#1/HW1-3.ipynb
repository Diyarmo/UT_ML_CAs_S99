{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of Wikipedia:\n",
    "\n",
    "    Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.\n",
    "\n",
    "In GD we take steps based on the gradient of the function, using the negative of gradient as a guide to find the local minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(θ) = \\frac{1}{2} \\sum_{i=1}^{q}(h_{θ}(x^{i}) - y^{i})^{2}$\n",
    "\n",
    "$\\nabla{J(θ)} = \\nabla\\frac{1}{2} \\sum_{i=1}^{q}(h_{θ}(x^{i}) - y^{i})^{2}$\n",
    "\n",
    "$\\nabla{J(θ)} = \\sum_{i=1}^{q}(h_{θ}(x^{i})-y^{i}) \\nabla(h_{θ}(x^{i})- y^{i})$\n",
    "\n",
    "$\\nabla{J(θ)} = \\sum_{i=1}^{q}(h_{θ}(x^{i})-y^{i}) \\nabla(tanh(w^{T}x + b))$\n",
    "\n",
    "$\\nabla{J(θ)} = \\sum_{i=1}^{q}(h_{θ}(x^{i})-y^{i}) sech^2(w^{T}x + b) \\nabla(w^{T}x + b)$\n",
    "\n",
    "$\\frac{d}{db}{J(θ)} = \\sum_{i=1}^{q}(h_{θ}(x^{i})-y^{i}) sech^2(w^{T}x + b)$\n",
    "\n",
    "\n",
    "$\\frac{d}{dw^{j}}{J(θ)} = \\sum_{i=1}^{q}(h_{θ}(x^{i})-y^{i}) sech^2(w^{T}x + b) x^{j}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$θ_{n+1} = θ_{n} - \\alpha\\nabla{J(θ_{n})}$\n",
    "\n",
    "$b_{n+1} = b_{n} - \\alpha\\frac{d}{db}{J(θ)}$\n",
    "\n",
    "$dw^{j}_{n+1} = dw^{j}_{n} - \\alpha\\frac{d}{dw^{j}}{J(θ)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using a too small learning rate causes the optimizer to converge slowly because at each step is very small and amound of change is insignificant \n",
    "\n",
    "also learning rate should not be very big because it would jump instead of taking steps and cost funtion would start to flunctuate and never converges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
