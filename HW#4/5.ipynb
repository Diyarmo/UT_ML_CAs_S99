{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import gensim\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from hmmlearn.hmm import  GaussianHMM, GMMHMM\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('tsa_train.csv')[['Sentiment', 'SentimentText']]\n",
    "train_data, valid_data = train_test_split(train_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def nltk2wn_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:                    \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))    \n",
    "    wn_tagged = map(lambda x: (x[0], nltk2wn_tag(x[1])), nltk_tagged)\n",
    "\n",
    "    res_words = []\n",
    "    for word, tag in wn_tagged:\n",
    "        if tag is None:                        \n",
    "            res_words.append(word)\n",
    "        else:\n",
    "            res_words.append(lemmatizer.lemmatize(word, tag))\n",
    "\n",
    "    return \" \".join(res_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_sep(data):\n",
    "    corpus = []\n",
    "    for i in range(2):\n",
    "        cat_data = data[data.Sentiment == i].SentimentText\n",
    "        corpus.append(cat_data)\n",
    "    return corpus\n",
    "\n",
    "train_corpus = sent_sep(train_data)\n",
    "valid_corpus = sent_sep(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAPPY_EMO = r\" ([xX;:]-?[dD)]|:-?[\\)]|[;:][pP]) \"\n",
    "SAD_EMO = r\" (:'?[/|\\(]) \"\n",
    "\n",
    "def transform(X):\n",
    "    X = X.str.replace(r\"@[a-zA-Z0-9_]* \", \" \")\n",
    "    \n",
    "    # Keeping only the word after the #    \n",
    "    X = X.str.replace(\"#\", \"\")\n",
    "    \n",
    "    X = X.str.replace(r\"[-\\.\\n]\", \"\")\n",
    "    \n",
    "    # Removing HTML garbag\n",
    "    X = X.str.replace(r\"&\\w+;\", \"\")\n",
    "    \n",
    "    # Removing links\n",
    "    X = X.str.replace(r\"https?://\\S*\", \"\")\n",
    "    \n",
    "    \n",
    "    # replace repeated letters with only two occurences\n",
    "    # heeeelllloooo => heelloo\n",
    "    X = X.str.replace(r\"(.)\\1+\", r\"\\1\\1\")\n",
    "    \n",
    "    # mark emoticons as happy or sad\n",
    "    X = X.str.replace(HAPPY_EMO, \" happyemoticons \")\n",
    "    X = X.str.replace(SAD_EMO, \" sademoticons \")\n",
    "    \n",
    "    X = X.str.lower()\n",
    "    X = X.str.translate(str.maketrans('', '', string.punctuation))\n",
    "    X = X.str.translate(str.maketrans('', '', '1234567890'))\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus):\n",
    "    new_corpus = []\n",
    "    for c in corpus:\n",
    "        new_c = transform(c)\n",
    "        new_c = new_c.apply(lemmatize_sentence)\n",
    "        new_c = new_c.apply(lambda x: \" \".join([w for w in x.split() if w not in stop_words]))\n",
    "        new_corpus.append(new_c)\n",
    "    return new_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean = clean_corpus(train_corpus)\n",
    "valid_clean = clean_corpus(valid_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_corpus(corpus):\n",
    "    joined_corpus = []\n",
    "    for c in corpus:\n",
    "        joined_corpus.append(\" \".join(c.values))\n",
    "    return joined_corpus\n",
    "train_joined = join_corpus(train_clean)\n",
    "valid_joined = join_corpus(valid_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_len = []\n",
    "X_valid_len = []\n",
    "for i in range(2):\n",
    "    X_train_len.append(np.array(list(map(lambda x: len(x.split()), train_clean[i]))))\n",
    "    X_valid_len.append(np.array(list(map(lambda x: len(x.split()), valid_clean[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [_text.split() for _text in train_clean[0]]\n",
    "documents += [_text.split() for _text in train_clean[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model = gensim.models.word2vec.Word2Vec(size=100, \n",
    "                                            window=5, \n",
    "                                            min_count=8, \n",
    "                                            workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5775338, 7589952)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(documents, total_examples=len(documents), epochs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "X_valid = []\n",
    "for i in range(2):\n",
    "    X_train.append(list(map(lambda x: [ list(wv[w]) for w in x.split() if w in wv], train_clean[i].values)))\n",
    "    X_valid.append(list(map(lambda x: [ list(wv[w]) for w in x.split() if w in wv], valid_clean[i].values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    X_train[i] = [x for x in X_train[i] if x != []]\n",
    "    X_valid[i] = [x for x in X_valid[i] if x != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_len = []\n",
    "X_valid_len = []\n",
    "for i in range(2):\n",
    "    X_train_len.append(np.array(list(map(lambda x: len(x), X_train[i]))))\n",
    "    X_valid_len.append(np.array(list(map(lambda x: len(x), X_valid[i]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flatten = []\n",
    "X_valid_flatten = []\n",
    "for i in range(2):\n",
    "    flatten = []\n",
    "    for l in X_train[i]:\n",
    "        flatten.extend(l)\n",
    "    X_train_flatten.append(np.array(flatten))\n",
    "    \n",
    "    flatten = []\n",
    "    for l in X_valid[i]:\n",
    "        flatten.extend(l)\n",
    "    X_valid_flatten.append(np.array(flatten))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = GaussianHMM(covariance_type='full')\n",
    "model1 = GaussianHMM(covariance_type='full')\n",
    "\n",
    "model0 = model0.fit(X_train_flatten[0], X_train_len[0])\n",
    "model1 = model1.fit(X_train_flatten[1], X_train_len[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([0]*len(X_valid_len[0]) + [1]*len(X_valid_len[1]))\n",
    "predicts = []\n",
    "\n",
    "for c in range(2):\n",
    "    idx = X_valid_len[c].cumsum()\n",
    "    for i in range(len(idx)):\n",
    "        if i==0:\n",
    "            x = X_valid_flatten[c][:idx[i]]\n",
    "        else:\n",
    "            x = X_valid_flatten[c][idx[i-1]:idx[i]]\n",
    "        predicts.append(np.argmax([model0.score(x), model1.score(x)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianHMM:\n",
      "confusion matrix:\n",
      "[[5031 2756]\n",
      " [2691 7222]]\n",
      "precision:\t 0.7237923431549409\n",
      "recall:\t 0.728538283062645\n"
     ]
    }
   ],
   "source": [
    "print(\"GaussianHMM:\")\n",
    "print('confusion matrix:')\n",
    "print(confusion_matrix(y_true=y_true, y_pred= predicts))\n",
    "print('precision:\\t', precision_score(y_true=y_true, y_pred= predicts))\n",
    "print('recall:\\t', recall_score(y_true=y_true, y_pred= predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6831457832848653"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6601912123677787"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51081"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_len[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
